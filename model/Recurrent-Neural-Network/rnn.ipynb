{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T20:40:01.408525Z",
     "start_time": "2025-04-06T20:40:01.396270Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # get the last time step's output\n",
    "        return out\n",
    "\n",
    "def train_RNN(data, features, target, date_col, n_ahead, year_test_start, year_test_end):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Select relevant columns\n",
    "    rel_col = features + [target] + [date_col]\n",
    "    data = data[rel_col]\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "\n",
    "    # Create lag features\n",
    "    env_lags = [1, 2, 3, 4]\n",
    "    cases_lag = range(n_ahead, 12)\n",
    "\n",
    "    for lag in env_lags:\n",
    "        for feature in features:\n",
    "            data[f'{feature}_lag_{lag}'] = data[feature].shift(lag)\n",
    "\n",
    "    for lag in cases_lag:\n",
    "        data[f'{target}_lag_{lag}'] = data[target].shift(lag)\n",
    "\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Split data\n",
    "    train_data = data[data[date_col] < year_test_start]\n",
    "    test_data = data[(data[date_col] >= year_test_start) & (data[date_col] < year_test_end)]\n",
    "\n",
    "    X_train = train_data.drop(columns=[target, date_col])\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data.drop(columns=[target, date_col])\n",
    "    y_test = test_data[target]\n",
    "\n",
    "    # Reshape inputs to [batch_size, sequence_length, input_size]\n",
    "    # Here we treat each row as a \"sequence\" of length 1\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Dataset and loader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    input_size = X_train_tensor.shape[2]\n",
    "    hidden_size = 64\n",
    "    model = RNNModel(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_X).squeeze()\n",
    "            loss = loss_fn(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor).squeeze().cpu().numpy()\n",
    "        actual = y_test_tensor.cpu().numpy()\n",
    "\n",
    "    # Round predictions to integers\n",
    "    predict_data = [int(round(x)) for x in predictions]\n",
    "\n",
    "    # Compute metrics\n",
    "    MAE = mean_absolute_error(actual, predict_data)\n",
    "    MSE = mean_squared_error(actual, predict_data)\n",
    "\n",
    "    return predict_data, MAE, MSE\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:40:01.532261Z",
     "start_time": "2025-04-06T20:40:01.529354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import models_utils"
   ],
   "id": "803ca8b511441a69",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:40:01.650115Z",
     "start_time": "2025-04-06T20:40:01.644998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "municipals = []\n",
    "with (open(\"../municipals.txt\", \"r\") as f):\n",
    "    for line in f:\n",
    "        municipals.append(line.strip())\n",
    "municipals"
   ],
   "id": "6173705fcc3fbfdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ajuy',\n",
       " 'Alimodian',\n",
       " 'Anilao',\n",
       " 'Badiangan',\n",
       " 'Balasan',\n",
       " 'Banate',\n",
       " 'Barotac Nuevo',\n",
       " 'Barotac Viejo',\n",
       " 'Batad',\n",
       " 'Bingawan',\n",
       " 'Cabatuan',\n",
       " 'Calinog',\n",
       " 'Carles',\n",
       " 'Passi City',\n",
       " 'Concepcion',\n",
       " 'Dingle',\n",
       " 'Duenas',\n",
       " 'Dumangas',\n",
       " 'Estancia',\n",
       " 'Guimbal',\n",
       " 'Iloilo City',\n",
       " 'Igbaras',\n",
       " 'Janiuay',\n",
       " 'Lambunao',\n",
       " 'Leganes',\n",
       " 'Lemery',\n",
       " 'Leon',\n",
       " 'Maasin',\n",
       " 'Miagao',\n",
       " 'Mina',\n",
       " 'New Lucena',\n",
       " 'Oton',\n",
       " 'Pavia',\n",
       " 'Pototan',\n",
       " 'San Dionisio',\n",
       " 'San Enrique',\n",
       " 'San Joaquin',\n",
       " 'San Rafael',\n",
       " 'Santa Barbara',\n",
       " 'Sara',\n",
       " 'Tigbauan',\n",
       " 'Tubungan',\n",
       " 'Zarraga']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:49:52.719664Z",
     "start_time": "2025-04-06T20:40:01.715662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_weeks_ahead = [1,2,3,4,8,12]\n",
    "for municipal in municipals:\n",
    "    for n in n_weeks_ahead:\n",
    "        municipal_df = pd.read_csv(f\"../../data/Merged Data/{municipal}_merged.csv\")\n",
    "        municipal_df[\"Year-Week\"] = pd.to_datetime(municipal_df[\"Year-Week\"])\n",
    "        predicted, MAE, MSE = train_RNN(municipal_df, features=[\"Temperature\", \"Precipitation\", \"Humidity\"], target=\"Cases\", date_col=\"Year-Week\", n_ahead=n, year_test_start=\"2023-01-01\", year_test_end=\"2024-12-31\")\n",
    "        models_utils.save_data(municipal, n, MSE, MAE, predicted, municipal_df)"
   ],
   "id": "a5e29b4a9ee42d9f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:49:52.767255Z",
     "start_time": "2025-04-06T20:49:52.765454Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3434c54a0814c1b6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
