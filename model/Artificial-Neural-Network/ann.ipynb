{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T20:04:55.179953Z",
     "start_time": "2025-04-06T20:04:55.176229Z"
    }
   },
   "source": "import pandas as pd\n",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:04:55.194900Z",
     "start_time": "2025-04-06T20:04:55.192207Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "39b2e4db5df3cda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:04:56.756030Z",
     "start_time": "2025-04-06T20:04:55.405864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropouts):\n",
    "        super().__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [48, 32, 19]\n",
    "        layers = [] # initialize layers\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0])) # first layer\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1])) # append each hidden layer\n",
    "        self.layers = nn.ModuleList(layers) # read more about module lists for pytorch\n",
    "        self.output = nn.Linear(hidden_sizes[len(hidden_sizes)-1], output_size)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for dropout in dropouts])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layers[0](x))\n",
    "        x = self.dropouts[0](x)\n",
    "        x = self.relu(self.layers[1](x))\n",
    "        x = self.dropouts[1](x)\n",
    "        x = self.relu(self.layers[2](x))\n",
    "        x = self.dropouts[2](x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ],
   "id": "441e0aadfc55dbc8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:04:56.769227Z",
     "start_time": "2025-04-06T20:04:56.761546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def train_ANN(data, features, target, date_col, n_ahead, year_test_start, year_test_end):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use GPU\n",
    "    # Select relevant columns\n",
    "    rel_col = features + [target] + [date_col]\n",
    "    data = data[rel_col]\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "    # Define lags\n",
    "    env_lags = [1,2,3,4]  # 4-week lag for environment features\n",
    "    cases_lag = range(n_ahead, 12)  # n to 11 week lags for target variable\n",
    "\n",
    "    # Create lagged features for environment and target variables\n",
    "    for lag in env_lags:\n",
    "        for feature in features:\n",
    "            data[f'{feature}_lag_{lag}'] = data[feature].shift(lag)\n",
    "\n",
    "    for lag in cases_lag:\n",
    "        data[f'{target}_lag_{lag}'] = data[target].shift(lag)\n",
    "\n",
    "    # Remove any rows with missing values due to lagging\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Initialize list for storing predictions\n",
    "    predict_data = []\n",
    "    actual_data = []\n",
    "    # Split data into training and testing sets\n",
    "    train_data = data[data[date_col] < year_test_start]\n",
    "    test_data = data[(data[date_col] >= year_test_start) & (data[date_col] < year_test_end)]\n",
    "\n",
    "    X_train = train_data.drop(columns=[target, date_col])\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data.drop(columns=[target, date_col])\n",
    "    y_test = test_data[target]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Combine inputs and labels into a Dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the ANN Model\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_sizes = [48, 32, 18]\n",
    "    dropouts = [0.3, 0.2, 0.1]\n",
    "    output_size = 1\n",
    "    model = Model(input_size, hidden_sizes, output_size, dropouts)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss_history = []\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model predictions\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Compute loss (MSE)\n",
    "            loss = loss_fn(predictions.squeeze(), batch_y)\n",
    "            loss_history.append(loss.item())\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test).squeeze().cpu().numpy()\n",
    "        actual_y = y_test.cpu().numpy()\n",
    "        predict_data.extend(predictions)\n",
    "        actual_data.extend(actual_y)\n",
    "    # Make predictions integer\n",
    "    predict_data = [int(round(x)) for x in predict_data]\n",
    "    # Calculate Mean Absolute Error\n",
    "    MAE = mean_absolute_error(actual_data, predict_data)\n",
    "    MSE = mean_squared_error(actual_data, predict_data)\n",
    "    return predict_data, MAE, MSE"
   ],
   "id": "cc7470185e9bc12b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:04:56.810617Z",
     "start_time": "2025-04-06T20:04:56.807463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import models_utils\n",
    "#Ajuy = pd.read_csv(\"../../data/Merged Data/Ajuy_merged.csv\")\n",
    "#Ajuy[\"Year-Week\"] = pd.to_datetime(Ajuy[\"Year-Week\"])\n",
    "#predicted, MAE, MSE = train_ANN(Ajuy, features=[\"Temperature\", \"Precipitation\", \"Humidity\"], target=\"Cases\", date_col=\"Year-Week\", n_ahead=1, year_test_start=\"2023-01-01\", year_test_end=\"2024-12-31\")"
   ],
   "id": "27019244ede6e40",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:04:56.859918Z",
     "start_time": "2025-04-06T20:04:56.853910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "municipals = []\n",
    "with (open(\"../municipals.txt\", \"r\") as f):\n",
    "    for line in f:\n",
    "        municipals.append(line.strip())\n",
    "municipals"
   ],
   "id": "30df5114ee1a1bd8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ajuy',\n",
       " 'Alimodian',\n",
       " 'Anilao',\n",
       " 'Badiangan',\n",
       " 'Balasan',\n",
       " 'Banate',\n",
       " 'Barotac Nuevo',\n",
       " 'Barotac Viejo',\n",
       " 'Batad',\n",
       " 'Bingawan',\n",
       " 'Cabatuan',\n",
       " 'Calinog',\n",
       " 'Carles',\n",
       " 'Passi City',\n",
       " 'Concepcion',\n",
       " 'Dingle',\n",
       " 'Duenas',\n",
       " 'Dumangas',\n",
       " 'Estancia',\n",
       " 'Guimbal',\n",
       " 'Iloilo City',\n",
       " 'Igbaras',\n",
       " 'Janiuay',\n",
       " 'Lambunao',\n",
       " 'Leganes',\n",
       " 'Lemery',\n",
       " 'Leon',\n",
       " 'Maasin',\n",
       " 'Miagao',\n",
       " 'Mina',\n",
       " 'New Lucena',\n",
       " 'Oton',\n",
       " 'Pavia',\n",
       " 'Pototan',\n",
       " 'San Dionisio',\n",
       " 'San Enrique',\n",
       " 'San Joaquin',\n",
       " 'San Rafael',\n",
       " 'Santa Barbara',\n",
       " 'Sara',\n",
       " 'Tigbauan',\n",
       " 'Tubungan',\n",
       " 'Zarraga']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:18:25.814264Z",
     "start_time": "2025-04-06T20:07:32.286948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_weeks_ahead = [1,2,3,4,8,12]\n",
    "for municipal in municipals:\n",
    "    for n in n_weeks_ahead:\n",
    "        municipal_df = pd.read_csv(f\"../../data/Merged Data/{municipal}_merged.csv\")\n",
    "        municipal_df[\"Year-Week\"] = pd.to_datetime(municipal_df[\"Year-Week\"])\n",
    "        predicted, MAE, MSE = train_ANN(municipal_df, features=[\"Temperature\", \"Precipitation\", \"Humidity\"], target=\"Cases\", date_col=\"Year-Week\", n_ahead=n, year_test_start=\"2023-01-01\", year_test_end=\"2024-12-31\")\n",
    "        models_utils.save_data(municipal, n, MSE, MAE, predicted, municipal_df)"
   ],
   "id": "294277be76b967cc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:07:16.361008508Z",
     "start_time": "2025-04-06T19:48:31.252227Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dc1d471cd5dd3011",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ajuy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m models_utils\u001B[38;5;241m.\u001B[39msave_data(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAjuy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m, MSE,MAE, predicted, \u001B[43mAjuy\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Ajuy' is not defined"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:07:16.361508120Z",
     "start_time": "2025-04-06T19:25:36.871973Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "35dc755b586b66d3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
