{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T19:29:56.204968Z",
     "start_time": "2025-04-09T19:29:56.201122Z"
    }
   },
   "source": "import pandas as pd\n",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T19:29:56.257063Z",
     "start_time": "2025-04-09T19:29:56.254198Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "39b2e4db5df3cda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T19:29:58.110644Z",
     "start_time": "2025-04-09T19:29:56.514897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropouts):\n",
    "        super().__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [48, 32, 19]\n",
    "        layers = [] # initialize layers\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0])) # first layer\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1])) # append each hidden layer\n",
    "        self.layers = nn.ModuleList(layers) # read more about module lists for pytorch\n",
    "        self.output = nn.Linear(hidden_sizes[len(hidden_sizes)-1], output_size)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for dropout in dropouts])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layers[0](x))\n",
    "        x = self.dropouts[0](x)\n",
    "        x = self.relu(self.layers[1](x))\n",
    "        x = self.dropouts[1](x)\n",
    "        x = self.relu(self.layers[2](x))\n",
    "        x = self.dropouts[2](x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ],
   "id": "441e0aadfc55dbc8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T19:29:58.125545Z",
     "start_time": "2025-04-09T19:29:58.115614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def train_ANN(data, features_lag, features_addtl, target, date_col, n_ahead, year_test_start, year_test_end):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use GPU\n",
    "    # Select relevant columns\n",
    "    rel_col = features_lag + [target] + [date_col]\n",
    "    data = data[rel_col]\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "    # Define lags\n",
    "    env_lags = [1,2,3,4]  # 4-week lag for environment features\n",
    "    cases_lag = range(n_ahead, 12+n_ahead)  # n to 11 week lags for target variable\n",
    "\n",
    "    # Create lagged features for environment and target variables\n",
    "    for lag in env_lags:\n",
    "        for feature in features_lag:\n",
    "            data[f'{feature}_lag_{lag}'] = data[feature].shift(lag)\n",
    "\n",
    "    for lag in cases_lag:\n",
    "        data[f'{target}_lag_{lag}'] = data[target].shift(lag)\n",
    "\n",
    "    # Remove any rows with missing values due to lagging\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Initialize list for storing predictions\n",
    "    predict_data = []\n",
    "    actual_data = []\n",
    "    # Split data into training and testing sets\n",
    "    train_data = data[data[date_col] < year_test_start]\n",
    "    test_data = data[(data[date_col] >= year_test_start) & (data[date_col] < year_test_end)]\n",
    "\n",
    "    X_train = train_data.drop(columns=[target, date_col])\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data.drop(columns=[target, date_col])\n",
    "    y_test = test_data[target]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Combine inputs and labels into a Dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the ANN Model\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_sizes = [48, 32, 18]\n",
    "    dropouts = [0.3, 0.2, 0.1]\n",
    "    output_size = 1\n",
    "    model = Model(input_size, hidden_sizes, output_size, dropouts)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss_history = []\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model predictions\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Compute loss (MSE)\n",
    "            loss = loss_fn(predictions.squeeze(), batch_y)\n",
    "            loss_history.append(loss.item())\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test).squeeze().cpu().numpy()\n",
    "        actual_y = y_test.cpu().numpy()\n",
    "        predict_data.extend(predictions)\n",
    "        actual_data.extend(actual_y)\n",
    "    # Make predictions integer\n",
    "    predict_data = [int(round(x)) for x in predict_data]\n",
    "    # Calculate Mean Absolute Error\n",
    "    MAE = mean_absolute_error(actual_data, predict_data)\n",
    "    MSE = mean_squared_error(actual_data, predict_data)\n",
    "    return predict_data, MAE, MSE"
   ],
   "id": "cc7470185e9bc12b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T19:29:58.173149Z",
     "start_time": "2025-04-09T19:29:58.169980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import models_utils\n",
    "#Ajuy = pd.read_csv(\"../../data/Merged Data/Ajuy_merged.csv\")\n",
    "#Ajuy[\"Year-Week\"] = pd.to_datetime(Ajuy[\"Year-Week\"])\n",
    "#predicted, MAE, MSE = train_ANN(Ajuy, features=[\"Temperature\", \"Precipitation\", \"Humidity\"], target=\"Cases\", date_col=\"Year-Week\", n_ahead=1, year_test_start=\"2023-01-01\", year_test_end=\"2024-12-31\")"
   ],
   "id": "27019244ede6e40",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T19:29:58.222455Z",
     "start_time": "2025-04-09T19:29:58.219471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "municipals = []\n",
    "with (open(\"../municipals.txt\", \"r\") as f):\n",
    "    for line in f:\n",
    "        municipals.append(line.strip())"
   ],
   "id": "30df5114ee1a1bd8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T21:07:06.943432Z",
     "start_time": "2025-04-09T20:40:51.043010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_weeks_ahead = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for municipal in municipals:\n",
    "    for n in n_weeks_ahead:\n",
    "        municipal_df = pd.read_csv(f\"../../data/Merged Data/{municipal}_merged.csv\")\n",
    "        municipal_df[\"Year-Week\"] = pd.to_datetime(municipal_df[\"Year-Week\"])\n",
    "        predicted, MAE, MSE = train_ANN(municipal_df, features_lag=[\"Temperature\", \"Precipitation\", \"Humidity\"], features_addtl = [\"Year\", \"Week\", \"Population\", \"Month\"], target=\"Cases\", date_col=\"Year-Week\", n_ahead=n, year_test_start=\"2023-01-01\", year_test_end=\"2024-12-31\")\n",
    "        models_utils.save_data(municipal, n, MSE, MAE, predicted, municipal_df, \"Municipal\")"
   ],
   "id": "294277be76b967cc",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T19:48:37.826997Z",
     "start_time": "2025-04-09T19:48:37.812192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def train_ANN_pooled(data, X_train, y_train):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use GPU\n",
    "    #\n",
    "    data = data.dropna()\n",
    "\n",
    "    predict_data = []\n",
    "    actual_data = []\n",
    "   \n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Combine inputs and labels into a Dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the ANN Model\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_sizes = [48, 32, 18]\n",
    "    dropouts = [0.3, 0.2, 0.1]\n",
    "    output_size = 1\n",
    "    model = Model(input_size, hidden_sizes, output_size, dropouts)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss_history = []\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model predictions\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Compute loss (MSE)\n",
    "            loss = loss_fn(predictions.squeeze(), batch_y)\n",
    "            loss_history.append(loss.item())\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def test_ANN_pooled(model, X_test, y_test):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use GPU\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test).squeeze().cpu().numpy()\n",
    "        actual_y = y_test.cpu().numpy()\n",
    "\n",
    "    # Round predictions to nearest integer\n",
    "    predict_data = [int(round(x)) for x in predictions]\n",
    "\n",
    "    # Calculate error metrics\n",
    "    MAE = mean_absolute_error(actual_y, predict_data)\n",
    "    MSE = mean_squared_error(actual_y, predict_data)\n",
    "\n",
    "    return predict_data, MAE, MSE\n"
   ],
   "id": "8485246643ba9f99",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T19:48:38.941258Z",
     "start_time": "2025-04-09T19:48:37.963614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pooled_df = pd.DataFrame()\n",
    "n = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for municipal in municipals:\n",
    "    municipal_df = pd.read_csv(f\"../../data/Merged Data/{municipal}_merged.csv\")\n",
    "    municipal_df[\"Municipal\"] = municipal\n",
    "    # lag the features\n",
    "    municipal_df = models_utils.prepare_dataframe(municipal_df, [\"Temperature\", \"Precipitation\", \"Humidity\"], [\"Year\", \"Week\", \"Month\", \"Population\"], \"Cases\", \"Year-Week\", 1)\n",
    "    pooled_df = pd.concat([pooled_df, municipal_df], axis=0)"
   ],
   "id": "dc1d471cd5dd3011",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T20:24:09.423761Z",
     "start_time": "2025-04-09T19:48:38.981714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_weeks_ahead = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for n in n_weeks_ahead:\n",
    "\n",
    "    pooled_df = pd.DataFrame()\n",
    "    for municipal in municipals:\n",
    "        municipal_df = pd.read_csv(f\"../../data/Merged Data/{municipal}_merged.csv\")\n",
    "        municipal_df[\"Municipal\"] = municipal\n",
    "        # lag the features\n",
    "        municipal_df = models_utils.prepare_dataframe(municipal_df, [\"Temperature\", \"Precipitation\", \"Humidity\"], [\"Year\", \"Week\", \"Month\", \"Population\"], \"Cases\", \"Year-Week\", n)\n",
    "        pooled_df = pd.concat([pooled_df, municipal_df], axis=0)\n",
    "\n",
    "    pooled_df.dropna()\n",
    "    pooled_X = pooled_df.drop(columns=[\"Cases\", \"Year-Week\"])\n",
    "    pooled_y = pooled_df[\"Cases\"]\n",
    "    pooled_X_train = pooled_X[pooled_df[\"Year-Week\"] < \"2023-01-01\"]\n",
    "    pooled_y_train = pooled_y[pooled_df[\"Year-Week\"] < \"2023-01-01\"]\n",
    "    model = train_ANN_pooled(pooled_df, pooled_X_train, pooled_y_train)\n",
    "    for municipal in municipals:\n",
    "        municipal_df = pd.read_csv(f\"../../data/Merged Data/{municipal}_merged.csv\")\n",
    "        municipal_df[\"Year-Week\"] = pd.to_datetime(municipal_df[\"Year-Week\"])\n",
    "        municipal_df = models_utils.prepare_dataframe(municipal_df, [\"Temperature\", \"Precipitation\", \"Humidity\"], [\"Year\", \"Week\", \"Month\", \"Population\"], \"Cases\", \"Year-Week\", n)\n",
    "        municipal_X = municipal_df.drop(columns=[\"Cases\", \"Year-Week\"])\n",
    "        municipal_y = municipal_df[\"Cases\"]\n",
    "        municipal_X_test = municipal_X[(municipal_df[\"Year-Week\"] >= \"2023-01-01\") & (municipal_df[\"Year-Week\"] < \"2024-12-31\")]\n",
    "        municipal_y_test = municipal_y[(municipal_df[\"Year-Week\"] >= \"2023-01-01\") & (municipal_df[\"Year-Week\"] < \"2024-12-31\")]\n",
    "        predicted, MAE, MSE = test_ANN_pooled(model, municipal_X_test, municipal_y_test)\n",
    "        models_utils.save_data(municipal, n, MSE, MAE, predicted, municipal_df, type = \"Provincial\")"
   ],
   "id": "862077d9bf72d225",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5630961615f99c2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
